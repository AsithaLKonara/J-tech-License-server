# âœ… Local Testing Checklist & Execution Guide

**Date**: January 16, 2026  
**Version**: 3.0.0  
**Status**: Ready for Testing

---

## ðŸ“‹ Pre-Testing Checklist

### Environment Verification
- [ ] Windows 10+ or equivalent OS
- [ ] Python 3.8+ installed and in PATH
- [ ] Node.js 16+ installed and in PATH
- [ ] npm 8+ installed
- [ ] Composer 2.0+ installed
- [ ] MySQL 8.0+ installed and running
- [ ] Git 2.30+ installed
- [ ] 4GB+ RAM available
- [ ] 10GB+ free disk space
- [ ] Administrative access to computer

### Repository Status
- [ ] Repository cloned successfully
- [ ] All submodules initialized: `git submodule update --init`
- [ ] Current branch: `main` or `develop`
- [ ] No uncommitted changes: `git status` shows clean
- [ ] 112 commits ahead verified
- [ ] `.gitignore` properly configured
- [ ] `.env` files created and configured

### Dependency Installation
- [ ] Python virtual environment created: `python -m venv venv`
- [ ] Python venv activated: `.\venv\Scripts\Activate.ps1`
- [ ] Python dependencies installed: `pip install -r requirements.txt`
- [ ] Node.js dependencies installed: `npm install` (in tests and web-dashboard)
- [ ] Composer dependencies installed: `composer install`
- [ ] All installations without errors or warnings

### Database Setup
- [ ] MySQL service running
- [ ] Upload bridge database exists
- [ ] Database migrations executed: `php artisan migrate`
- [ ] Test data seeded: `php artisan db:seed --class=TestDataSeeder`
- [ ] Database verified with sample queries
- [ ] Database connection from app verified

### Configuration Files
- [ ] `.env` file in web-dashboard configured
- [ ] `.env` file in upload-bridge configured
- [ ] `APP_KEY` generated: `php artisan key:generate`
- [ ] Database credentials correct
- [ ] API URLs correct (http://localhost:8000, http://localhost:5000)
- [ ] JWT secrets configured
- [ ] CORS settings appropriate for localhost

---

## ðŸ§ª Unit Testing Phase

### Step 1: Core Module Tests

**Expected Duration**: 20 minutes

```powershell
cd apps/upload-bridge

# Run all unit tests
pytest tests/unit -v --tb=short

# Or run specific test files
pytest tests/unit/test_auth_manager.py -v
pytest tests/unit/test_gradient.py -v
pytest tests/unit/test_rate_limiter.py -v
pytest tests/unit/test_retry_utils.py -v
pytest tests/unit/test_network_validation.py -v
```

**Verification Checklist**:
- [ ] test_auth_manager.py: All tests pass
- [ ] test_gradient.py: All tests pass
- [ ] test_rate_limiter.py: All tests pass
- [ ] test_retry_utils.py: All tests pass
- [ ] test_network_validation.py: All tests pass
- [ ] test_connection_pool.py: All tests pass
- [ ] No errors or warnings in output
- [ ] Total runtime < 60 seconds
- [ ] Code coverage >= 95%

### Step 2: UI Component Tests

**Expected Duration**: 15 minutes

```powershell
# Canvas and widget tests
pytest tests/unit/test_matrix_design_canvas.py -v
pytest tests/unit/test_gradient_widget.py -v
pytest tests/unit/test_color_picker.py -v
pytest tests/unit/test_preset_manager_widget.py -v
```

**Verification Checklist**:
- [ ] test_matrix_design_canvas.py: All tests pass
- [ ] test_gradient_widget.py: All tests pass
- [ ] test_color_picker.py: All tests pass
- [ ] test_preset_manager_widget.py: All tests pass
- [ ] All drawing modes functional
- [ ] All event handlers work correctly
- [ ] No GUI-related failures
- [ ] Total runtime < 45 seconds

### Step 3: Parser Tests

**Expected Duration**: 10 minutes

```powershell
# Pattern parser tests
pytest tests/unit/test_standard_format_parser.py -v
pytest tests/unit/test_preset_parser.py -v
pytest tests/unit/test_animation_parser.py -v
```

**Verification Checklist**:
- [ ] test_standard_format_parser.py: All tests pass
- [ ] test_preset_parser.py: All tests pass
- [ ] test_animation_parser.py: All tests pass
- [ ] JSON parsing robust
- [ ] Error handling covers edge cases
- [ ] Total runtime < 30 seconds

### Unit Tests Success Criteria

```
âœ… Pass Rate: >= 99%
âœ… Code Coverage: >= 95%
âœ… No Errors: 0 failed tests
âœ… No Warnings: Clean output
âœ… Duration: < 120 seconds
```

---

## ðŸ”— Integration Testing Phase

### Step 1: Start Web Server

**Expected Duration**: 5 minutes

```powershell
# Terminal 1: Start Laravel web server
cd apps/web-dashboard
php artisan serve

# Verify server is running
# Should see: "Server running on [http://127.0.0.1:8000]"

# Terminal 2: Test API endpoints
curl -X GET http://localhost:8000/api/v2/health
# Expected response: {"status":"ok"}
```

**Verification Checklist**:
- [ ] Web server starts without errors
- [ ] Server listens on http://127.0.0.1:8000
- [ ] Health endpoint responds
- [ ] Database connection successful
- [ ] Static assets served correctly

### Step 2: API Communication Tests

**Expected Duration**: 20 minutes

```powershell
# Terminal 3: Run integration tests (while server running)
cd apps/upload-bridge
pytest tests/integration/test_api_communication.py -v
```

**Test Cases**:
- [ ] Health check endpoint responds
- [ ] User registration endpoint works
- [ ] User login endpoint works
- [ ] License validation endpoint works
- [ ] Device registration endpoint works
- [ ] Token refresh endpoint works
- [ ] Error handling correct
- [ ] Response codes correct

**Verification Checklist**:
- [ ] All 8+ API tests pass
- [ ] No timeout errors
- [ ] No connection errors
- [ ] Response times < 200ms
- [ ] JSON validation passes

### Step 3: Pattern Upload Workflow

**Expected Duration**: 20 minutes

```powershell
pytest tests/integration/test_upload_workflow.py -v
```

**Test Cases**:
- [ ] Pattern validation works
- [ ] Binary encoding correct
- [ ] Temp file creation/cleanup
- [ ] Progress callbacks fire
- [ ] Error handling for corrupted data
- [ ] Retry logic works
- [ ] Rate limiting enforced
- [ ] Upload completion verified

**Verification Checklist**:
- [ ] All 8+ workflow tests pass
- [ ] No orphaned temp files
- [ ] Proper resource cleanup
- [ ] Progress reports accurate
- [ ] Error messages helpful

### Step 4: License Management

**Expected Duration**: 15 minutes

```powershell
pytest tests/integration/test_license_workflow.py -v
```

**Test Cases**:
- [ ] License key validation
- [ ] Device seat management
- [ ] Offline grace period tracking
- [ ] Subscription tier enforcement
- [ ] Feature entitlement checking
- [ ] License renewal process
- [ ] Hardware binding verification

**Verification Checklist**:
- [ ] All 7+ license tests pass
- [ ] License state persists correctly
- [ ] Grace period countdown accurate
- [ ] Seat limits enforced
- [ ] Offline mode functions

### Integration Tests Success Criteria

```
âœ… Pass Rate: >= 98%
âœ… All 5+ Test Files Pass
âœ… No Timeout Errors
âœ… API Response Time: < 200ms average
âœ… Database State Clean Between Tests
âœ… Duration: < 180 seconds
```

---

## ðŸŒ End-to-End Testing Phase

### Step 1: Setup E2E Environment

**Expected Duration**: 10 minutes

```powershell
# Terminal 1: Start web server (if not already running)
cd apps/web-dashboard
php artisan serve

# Terminal 2: Check test dependencies
cd tests
npm install

# Terminal 3: Verify test data loaded
cd ../apps/web-dashboard
php artisan tinker
# In tinker shell:
# User::count()  # Should return > 5
# License::count()  # Should return > 5
# Device::count()  # Should return 0 (devices registered during tests)
```

**Verification Checklist**:
- [ ] Web server running
- [ ] Test dependencies installed
- [ ] Test data exists in database
- [ ] No test user is logged in
- [ ] Database backups created

### Step 2: Basic Authentication E2E

**Expected Duration**: 15 minutes

```powershell
cd tests
npm test -- tests/e2e/basic-auth.test.js
```

**Test Scenarios**:
- [ ] User registration (valid data)
- [ ] User registration (invalid data)
- [ ] User login (valid credentials)
- [ ] User login (invalid credentials)
- [ ] User logout
- [ ] Password reset workflow
- [ ] Email verification

**Verification Checklist**:
- [ ] All 7+ auth tests pass
- [ ] No console errors
- [ ] Proper redirects
- [ ] Error messages display correctly
- [ ] Sessions work properly

### Step 3: License Activation E2E

**Expected Duration**: 20 minutes

```powershell
npm test -- tests/e2e/license-activation.test.js
```

**Test Scenarios**:
- [ ] Request trial license
- [ ] Validate license key format
- [ ] Activate license
- [ ] Verify device binding
- [ ] Check license status
- [ ] Renew expired license
- [ ] View device list
- [ ] Manage device seats

**Verification Checklist**:
- [ ] All 8+ license tests pass
- [ ] License appears in dashboard
- [ ] Device binding works
- [ ] Seat limits enforced
- [ ] License status accurate

### Step 4: Desktop App Integration E2E

**Expected Duration**: 30 minutes

```powershell
# Terminal 1: Start web server (keep running)

# Terminal 2: Start desktop app
cd apps/upload-bridge
python main.py

# Terminal 3: Run integration tests (watch for desktop app responses)
cd tests
npm test -- tests/e2e/desktop-integration.test.js
```

**Test Scenarios**:
- [ ] App launches successfully
- [ ] License validation works
- [ ] API connection established
- [ ] Device discovery works
- [ ] Network connectivity tests
- [ ] Offline detection works
- [ ] Error recovery works

**Verification Checklist**:
- [ ] All 7+ desktop tests pass
- [ ] App doesn't freeze
- [ ] Network calls succeed
- [ ] Device list updates
- [ ] Timeout handling works

### Step 5: Complete User Workflow E2E

**Expected Duration**: 45 minutes

```powershell
npm test -- tests/e2e/complete-workflow.test.js
```

**Workflow Steps**:

1. **Registration & Activation** (10 min)
   - [ ] New user registers
   - [ ] Email confirmed
   - [ ] User logs in
   - [ ] Trial license requested
   - [ ] License key activated
   - [ ] Device bound to account

2. **Pattern Design** (15 min)
   - [ ] Desktop app launched
   - [ ] License validated
   - [ ] New pattern created (32x32 grid)
   - [ ] Canvas responsive to mouse
   - [ ] Drawing tools work
   - [ ] Gradient tool works
   - [ ] Shape tool works
   - [ ] Undo/redo works

3. **Simulation** (10 min)
   - [ ] Simulation starts
   - [ ] Preview shows pattern
   - [ ] Animation smooth (60 FPS)
   - [ ] Frame controls work
   - [ ] Speed adjustment works
   - [ ] Simulation stops cleanly

4. **Upload** (10 min)
   - [ ] Device scanning works
   - [ ] Device connects
   - [ ] Pattern uploads
   - [ ] Progress bar displays
   - [ ] Upload completes
   - [ ] Dashboard shows upload history

**Verification Checklist**:
- [ ] All workflow steps complete successfully
- [ ] No data loss
- [ ] Progress tracking accurate
- [ ] All confirmations shown
- [ ] Dashboard reflects changes

### E2E Tests Success Criteria

```
âœ… Pass Rate: >= 95%
âœ… All Test Scenarios Pass
âœ… No Flaky Tests (consistent results)
âœ… User Workflows Complete Successfully
âœ… Data Persistence Verified
âœ… Error Recovery Works
âœ… Duration: < 180 seconds
```

---

## âš¡ Performance Testing Phase

### Step 1: Canvas Performance

**Expected Duration**: 15 minutes

```powershell
cd apps/upload-bridge
pytest tests/performance/test_canvas_performance.py -v
```

**Metrics**:
- [ ] Rendering 10,000 pixels: < 500ms
- [ ] Memory usage: < 500MB sustained
- [ ] Undo/redo operation: < 100ms
- [ ] Gradient computation: < 200ms
- [ ] Zoom operation: < 50ms
- [ ] Pan operation: < 50ms

**Verification Checklist**:
- [ ] All performance tests pass
- [ ] No memory leaks detected
- [ ] No CPU spikes
- [ ] UI remains responsive

### Step 2: Network Performance

**Expected Duration**: 15 minutes

```powershell
pytest tests/performance/test_upload_performance.py -v
```

**Metrics**:
- [ ] Connection pooling reduces overhead by 30%+
- [ ] Upload speed > 5 MB/s on LAN
- [ ] API response time < 200ms average
- [ ] Database queries < 100ms average
- [ ] Rate limiter overhead < 5%
- [ ] Retry logic doesn't cause timeouts

**Verification Checklist**:
- [ ] All performance tests pass
- [ ] Network metrics meet targets
- [ ] No connection leaks
- [ ] Resource cleanup proper

### Step 3: Load Testing

**Expected Duration**: 15 minutes

```powershell
cd tests/load
node load-test-dashboard.js
node load-test-api.js
```

**Scenarios**:
- [ ] 10 concurrent users registering
- [ ] 5 concurrent uploads
- [ ] 100+ API requests/second
- [ ] Sustained load for 5 minutes
- [ ] CPU usage < 80%
- [ ] Memory usage stable

**Verification Checklist**:
- [ ] No dropped requests
- [ ] Response times remain consistent
- [ ] Database handles concurrency
- [ ] No race conditions detected

### Performance Tests Success Criteria

```
âœ… Canvas Rendering: < 500ms for 10k pixels
âœ… Upload Speed: > 5 MB/s on LAN
âœ… API Response: < 200ms average
âœ… Memory: < 500MB sustained
âœ… Concurrency: Handles 10+ simultaneous operations
âœ… CPU: Stays below 80% under normal load
```

---

## ðŸ”’ Security Testing Phase

**Optional but Recommended**: 45 minutes

```powershell
cd tests
npm test -- tests/security/
```

**Test Cases**:
- [ ] Invalid token rejected
- [ ] Expired token rejected
- [ ] SQL injection attempts blocked
- [ ] XSS attempts blocked
- [ ] CSRF protection enabled
- [ ] Rate limits prevent abuse
- [ ] Offline license expiration enforced
- [ ] Device binding validates correctly
- [ ] Pattern data validation prevents crashes
- [ ] File upload size limits enforced

**Verification Checklist**:
- [ ] All security tests pass
- [ ] No vulnerabilities identified
- [ ] Protection headers present
- [ ] Input validation robust

---

## ðŸ“Š Final Verification & Reporting

### Step 1: Generate Coverage Report

```powershell
# In apps/upload-bridge directory
pytest --cov=. --cov-report=html tests/

# Results saved to htmlcov/index.html
# Open in browser: start htmlcov/index.html
```

**Coverage Goals**:
- [ ] Overall coverage: >= 90%
- [ ] Core modules: >= 95%
- [ ] UI components: >= 85%
- [ ] Parsers: >= 90%

### Step 2: Collect Test Results

```powershell
# Create test report
$timestamp = Get-Date -Format "yyyyMMdd_HHmmss"
$report = "test-results-$timestamp.md"

# List all test files and results
ls test-reports/ | Out-File -Append $report
pytest --collect-only tests/ >> $report
```

### Step 3: Document Any Issues

**Issue Template**:
```markdown
## Issue #123

**Title**: [Brief description]
**Severity**: [Critical/High/Medium/Low]
**Component**: [Desktop App/Web Dashboard/API]
**Test**: [Which test found this]

### Steps to Reproduce
1. ...
2. ...
3. ...

### Expected Result
...

### Actual Result
...

### Logs
[Include relevant logs]

### Suggested Fix
...
```

### Step 4: Create Final Report

```markdown
# Testing Completion Report

**Date**: [Date]
**Environment**: Windows Local Development
**Version**: 3.0.0

## Summary
- Total Tests Run: [Count]
- Passed: [Count]
- Failed: [Count]
- Skipped: [Count]
- Duration: [Time]

## Coverage
- Overall: [%]
- Core Modules: [%]
- UI Components: [%]
- Parsers: [%]

## Issues Found
- Critical: [Count]
- High: [Count]
- Medium: [Count]
- Low: [Count]

## Recommendations
1. ...
2. ...
3. ...

## Sign-off
- QA Engineer: _______________
- Date: _______________
```

---

## ðŸš€ Quick Reference Commands

### Start Web Server
```powershell
cd apps/web-dashboard
php artisan serve
```

### Start Desktop App
```powershell
cd apps/upload-bridge
python main.py
```

### Run All Tests
```powershell
cd apps/upload-bridge
pytest tests/ -v
```

### Run Specific Test File
```powershell
pytest tests/unit/test_auth_manager.py -v
```

### Run Tests with Coverage
```powershell
pytest tests/ --cov=. --cov-report=html
```

### Generate HTML Coverage Report
```powershell
start htmlcov/index.html
```

### Check API Health
```powershell
curl -X GET http://localhost:8000/api/v2/health
```

### View Database
```powershell
mysql -u root -D upload_bridge
# SELECT * FROM users LIMIT 5;
```

### Clear Database
```powershell
cd apps/web-dashboard
php artisan migrate:refresh --force
php artisan db:seed --class=TestDataSeeder
```

---

## âœ¨ Testing Best Practices

1. **Isolation**: Each test should be independent and not rely on other tests
2. **Repeatability**: Tests should produce same results every time
3. **Clarity**: Test names should describe what they test
4. **Speed**: Aim for < 5 seconds per test
5. **Coverage**: Aim for >= 90% code coverage
6. **Cleanup**: Always clean up resources (files, database, etc.)
7. **Monitoring**: Watch for flaky tests and fix them
8. **Documentation**: Comment complex test logic

---

## ðŸŽ¯ Success Criteria Summary

| Phase | Tests | Pass Rate | Duration | Status |
|-------|-------|-----------|----------|--------|
| Unit | 85+ | â‰¥99% | <2 min | âœ… |
| Integration | 50+ | â‰¥98% | <5 min | âœ… |
| E2E | 25+ | â‰¥95% | <10 min | âœ… |
| Performance | 15+ | â‰¥90% | <3 min | âœ… |
| **Total** | **175+** | **â‰¥96%** | **<20 min** | **âœ…** |

---

## ðŸ“ž Support & Troubleshooting

See [COMPLETE_LOCAL_TESTING_PLAN.md](COMPLETE_LOCAL_TESTING_PLAN.md) for detailed troubleshooting guide.

**Common Issues**:
- Virtual environment not activated â†’ See Prerequisites
- MySQL not running â†’ `.\scripts\mysql-start.ps1`
- Port 8000 in use â†’ Kill process or use different port
- Database migration failure â†’ Run `php artisan migrate:refresh --force`
- Tests timeout â†’ Increase timeout in pytest.ini

---

**Ready to Start Testing**: January 16, 2026  
**Last Updated**: January 16, 2026  
**Status**: âœ… Complete and Ready
